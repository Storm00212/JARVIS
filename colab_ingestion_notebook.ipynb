{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Storm00212/JARVIS/blob/main/colab_ingestion_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "930bb34e",
      "metadata": {
        "id": "930bb34e"
      },
      "source": [
        "\n",
        "# JARVIS RAG Ingestion Notebook (Colab-ready)\n",
        "\n",
        "**Purpose:** This notebook walks you through an end-to-end prototype ingestion pipeline that:\n",
        "- Accepts PDF / DOCX / PPTX documents\n",
        "- Extracts clean text (with optional OCR)\n",
        "- Splits documents into semantic chunks\n",
        "- Generates embeddings for chunks\n",
        "- Stores chunks + embeddings into a local Chroma vector store\n",
        "- Exposes a simple `ask(question)` function that uses retrieval + prompt assembly (RAG)\n",
        "\n",
        "**Notes & assumptions**\n",
        "- Designed for Google Colab interactive use.\n",
        "- Includes a sample path from this session: `/mnt/data/jarvis-ai.zip` which you can inspect or replace with your own uploads.\n",
        "- Each code cell includes detailed comments to help you follow along.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7dd3208",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7dd3208",
        "outputId": "71f402ad-2365-4625-d28f-761ec9e21f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencies installed (or already present).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# SECTION 1: Install required packages\n",
        "# Run this cell in Google Colab to install dependencies. It may take 1-2 minutes.\n",
        "!pip install --quiet pypdf python-docx python-pptx sentence-transformers chromadb langchain tiktoken\n",
        "print('Dependencies installed (or already present).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb1df0a",
      "metadata": {
        "id": "cdb1df0a"
      },
      "source": [
        "\n",
        "## SECTION 2: Upload files (use UI) or use sample path\n",
        "\n",
        "You can upload files interactively using the cell below, or skip upload and use the sample file `'/mnt/data/jarvis-ai.zip'` if present.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6ERR0AEhhA6",
        "outputId": "542eeaa7-e49b-4c5c-cb94-fbbcabd8ca63"
      },
      "id": "o6ERR0AEhhA6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the directory to upload the files\n",
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/jarvis-ai\"\n",
        "RAW_DATA_DIR = f\"{BASE_DIR}/data/raw\"\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Base project folder:\", BASE_DIR)\n",
        "print(\"Raw data folder:\", RAW_DATA_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvxHIzeqiCkB",
        "outputId": "1a464389-f536-47d3-da1f-4caa96cc77f4"
      },
      "id": "IvxHIzeqiCkB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base project folder: /content/drive/MyDrive/jarvis-ai\n",
            "Raw data folder: /content/drive/MyDrive/jarvis-ai/data/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uploading files to directory\n",
        "from google.colab import files\n",
        "import shutil # Import shutil for cross-device moves\n",
        "\n",
        "uploaded_files = files.upload()  # choose multiple files\n",
        "\n",
        "\n",
        "# Move uploaded files into the Drive folder\n",
        "for filename in uploaded_files.keys():\n",
        "    src = f\"/content/{filename}\"\n",
        "    dst = f\"{RAW_DATA_DIR}/{filename}\"\n",
        "    print(f\"Moving {src} → {dst}\")\n",
        "    # Use shutil.move to handle cross-device links (copy then delete)\n",
        "    shutil.move(src, dst)\n",
        "\n",
        "print(\"\\nUpload complete!\")\n",
        "\n",
        "print(\"Files in your study notes folder:\")\n",
        "print(os.listdir(RAW_DATA_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HD04XWpniZKa",
        "outputId": "100cb336-95e5-411e-8ee2-0d78a80b7c1a"
      },
      "id": "HD04XWpniZKa",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f85d9b13-fae3-41d8-8917-768b52e91052\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f85d9b13-fae3-41d8-8917-768b52e91052\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1. Amplifiers with Negative Feedback.pdf to 1. Amplifiers with Negative Feedback.pdf\n",
            "Saving 3.1 Resources .pdf to 3.1 Resources .pdf\n",
            "Saving 3.2 Past Papers  .pdf to 3.2 Past Papers  .pdf\n",
            "Saving A textbook of Electrical Technology B. L. Thereja All Volumes ( PDFDrive.pdf to A textbook of Electrical Technology B. L. Thereja All Volumes ( PDFDrive.pdf\n",
            "Saving applied-numerical-methods-with-matlab-for-engineers-and-scientists-4nbsped-0073397962-9780073397962_compress.pdf to applied-numerical-methods-with-matlab-for-engineers-and-scientists-4nbsped-0073397962-9780073397962_compress.pdf\n",
            "Saving assignment_1.pdf to assignment_1.pdf\n",
            "Saving cat.ii.q5.revised.solution.png to cat.ii.q5.revised.solution.png\n",
            "Saving churchillbrown.pdf to churchillbrown.pdf\n",
            "Saving Complex analysis Q&A.pdf to Complex analysis Q&A.pdf\n",
            "Saving Complex analysis Q&A2.pdf to Complex analysis Q&A2.pdf\n",
            "Saving Design_of_Analog_Filters_Rolf_Schaumann.pdf to Design_of_Analog_Filters_Rolf_Schaumann.pdf\n",
            "Saving digielec.pdf to digielec.pdf\n",
            "Saving DOC-20250827-WA0009..pdf to DOC-20250827-WA0009..pdf\n",
            "Saving DOC-20250908-WA0053..pdf to DOC-20250908-WA0053..pdf\n",
            "Saving Document from ..pdf to Document from ..pdf\n",
            "Saving EEE 2206_EET 2204_Electromagnetics I_Exam.pdf to EEE 2206_EET 2204_Electromagnetics I_Exam.pdf\n",
            "Saving EEE 2305 - Electromagnetics I - Exam - 2017-18 sem I (2).pdf to EEE 2305 - Electromagnetics I - Exam - 2017-18 sem I (2).pdf\n",
            "Saving EEE 3101_Analogue Electronics_Exam.pdf to EEE 3101_Analogue Electronics_Exam.pdf\n",
            "Saving EEE 3101cbd_ANALOGUE ELECTRONICS I EXAM 2019-2020.pdf to EEE 3101cbd_ANALOGUE ELECTRONICS I EXAM 2019-2020.pdf\n",
            "Saving EEE 3103 ELECTROMAGNETICS II.pdf to EEE 3103 ELECTROMAGNETICS II.pdf\n",
            "Saving EEE 3207 ELECTRICAL MACHINES 2.pptx to EEE 3207 ELECTRICAL MACHINES 2.pptx\n",
            "Saving EEE 3208 ELECTROMAGNETICS III lec1 notes.pdf to EEE 3208 ELECTROMAGNETICS III lec1 notes.pdf\n",
            "Saving EEE 3208 ELECTROMAGNETICS IIILecture 2 3 and4 notes (1).pdf to EEE 3208 ELECTROMAGNETICS IIILecture 2 3 and4 notes (1).pdf\n",
            "Saving EEE 3208 ELECTROMAGNETICS IIILecture 2 3 and4 notes.pdf to EEE 3208 ELECTROMAGNETICS IIILecture 2 3 and4 notes.pdf\n",
            "Saving eee.eti.3104.cat.ii.make_up.ms.pdf to eee.eti.3104.cat.ii.make_up.ms.pdf\n",
            "Saving eee.eti.3104.model.solutions.cat.ii.pdf to eee.eti.3104.model.solutions.cat.ii.pdf\n",
            "Saving EEE_ETI 3101_SUP_EXAM_ANALOGUE ELECTRONICS 1 ready.pdf to EEE_ETI 3101_SUP_EXAM_ANALOGUE ELECTRONICS 1 ready.pdf\n",
            "Saving EEE_ETI 3101_SUP_EXAM_ANALOGUE ELECTRONICS 1.pdf to EEE_ETI 3101_SUP_EXAM_ANALOGUE ELECTRONICS 1.pdf\n",
            "Saving EEE_ETI3105_Assignment ONE.pdf to EEE_ETI3105_Assignment ONE.pdf\n",
            "Saving EEE_ETI3105_CAT 2.pdf to EEE_ETI3105_CAT 2.pdf\n",
            "Saving EEE2205 Electromagnetics I ready.pdf to EEE2205 Electromagnetics I ready.pdf\n",
            "Saving EEE2205 Electromagnetics I.pdf to EEE2205 Electromagnetics I.pdf\n",
            "Saving EEE2206cbd ELECTROMAGNETICS I.pdf to EEE2206cbd ELECTROMAGNETICS I.pdf\n",
            "Saving eee3102 [1-20].pdf to eee3102 [1-20].pdf\n",
            "Saving eee3102 [21-33].pdf to eee3102 [21-33].pdf\n",
            "Saving eee3102 [49-67].pdf to eee3102 [49-67].pdf\n",
            "Saving eee3104eti3104 [1-68].pdf to eee3104eti3104 [1-68].pdf\n",
            "Saving eee3104eti3104.pdf to eee3104eti3104.pdf\n",
            "Saving elecmag-i-electromagnetics....pdf to elecmag-i-electromagnetics....pdf\n",
            "Saving Electromagnetic_Field_Theory_U_A_Bakshi. - By EasyEngineering.net.pdf to Electromagnetic_Field_Theory_U_A_Bakshi. - By EasyEngineering.net.pdf\n",
            "Saving Electromagnetics.pdf to Electromagnetics.pdf\n",
            "Saving Electronics Fundamentals Circuits Devices and Applications 8th Edition By David M Buchla and Thomas L Floyd.pdf to Electronics Fundamentals Circuits Devices and Applications 8th Edition By David M Buchla and Thomas L Floyd.pdf\n",
            "Saving guru_b-s-_hiziroglu_h-r-_electromagnetic_field_bookzz-org.pdf to guru_b-s-_hiziroglu_h-r-_electromagnetic_field_bookzz-org.pdf\n",
            "Saving Lecture 1_BJT Configurations & Their Characteristics.pdf to Lecture 1_BJT Configurations & Their Characteristics.pdf\n",
            "Saving LECTURE 1-MAGNETIC CIRCUITS.pdf to LECTURE 1-MAGNETIC CIRCUITS.pdf\n",
            "Saving Lecture 2_Large Signal Amplifiers_1 of 2.pdf to Lecture 2_Large Signal Amplifiers_1 of 2.pdf\n",
            "Saving LECTURE 2-TRANSFORMERS.pdf to LECTURE 2-TRANSFORMERS.pdf\n",
            "Saving Lecture 3_Large Signal Amplifiers_2 of 2.pdf to Lecture 3_Large Signal Amplifiers_2 of 2.pdf\n",
            "Saving LECTURE 3-PRACTICAL TRANSFORMERS.pdf to LECTURE 3-PRACTICAL TRANSFORMERS.pdf\n",
            "Saving LECTURE 4-EQUIVALENT CIRCUITS_TRANSFORMERS .pdf to LECTURE 4-EQUIVALENT CIRCUITS_TRANSFORMERS .pdf\n",
            "Saving Lecture 05_Three Phase AC circuits.pdf to Lecture 05_Three Phase AC circuits.pdf\n",
            "Saving LECTURE 5-TRANSFORMER TESTS & EFFICIENCY.pdf to LECTURE 5-TRANSFORMER TESTS & EFFICIENCY.pdf\n",
            "Saving LECTURE 6-DC MACHINE FUNDAMENTALS.pdf to LECTURE 6-DC MACHINE FUNDAMENTALS.pdf\n",
            "Saving Lecture 7-Filters.pdf to Lecture 7-Filters.pdf\n",
            "Saving LECTURE 7-TYPES OF DC GENERATORS.pdf to LECTURE 7-TYPES OF DC GENERATORS.pdf\n",
            "Saving LECTURE 8-DC MOTORS.pdf to LECTURE 8-DC MOTORS.pdf\n",
            "Saving Lecture 8-Laplace Transform.pdf to Lecture 8-Laplace Transform.pdf\n",
            "Saving MAGNETIC FLUX DENSITY.pdf to MAGNETIC FLUX DENSITY.pdf\n",
            "Saving model.solutions.pdf to model.solutions.pdf\n",
            "Saving Numerical-Methods-Rao-V.-Dukkipati-2010.pdf to Numerical-Methods-Rao-V.-Dukkipati-2010.pdf\n",
            "Saving previous.material.zip to previous.material.zip\n",
            "Saving Q1to4.pdf to Q1to4.pdf\n",
            "Saving Q5.pdf to Q5.pdf\n",
            "Saving Sadiqu.pdf to Sadiqu.pdf\n",
            "Saving SMA 3121 Topic 4 Cauchy formular (1).pdf to SMA 3121 Topic 4 Cauchy formular (1).pdf\n",
            "Saving SMA 3121 Topic 7 Laurent series (1).pdf to SMA 3121 Topic 7 Laurent series (1).pdf\n",
            "Saving SMA 3121 Topic 9 Using Residue (1).pdf to SMA 3121 Topic 9 Using Residue (1).pdf\n",
            "Saving SMA 3121 Topic 10 Conformal (1).pdf to SMA 3121 Topic 10 Conformal (1).pdf\n",
            "Saving startup architecture.jpg to startup architecture.jpg\n",
            "Saving Topic 2_Small Signal Amplifiers.pdf to Topic 2_Small Signal Amplifiers.pdf\n",
            "Saving Topic 3_Bipolar Junction Transistors (2).pdf to Topic 3_Bipolar Junction Transistors (2).pdf\n",
            "Saving Topic 3_Multistage Transistor Amplifiers.pdf to Topic 3_Multistage Transistor Amplifiers.pdf\n",
            "Saving Topic 3_Single Stage Transistor Amplifiers (1).pdf to Topic 3_Single Stage Transistor Amplifiers (1).pdf\n",
            "Saving Topic 3_Single Stage Transistor Amplifiers.pdf to Topic 3_Single Stage Transistor Amplifiers.pdf\n",
            "Saving Topic 4_MOSFETS.pdf to Topic 4_MOSFETS.pdf\n",
            "Saving V.K-Mehta-Principles-of-Electronics.pdf to V.K-Mehta-Principles-of-Electronics.pdf\n",
            "Saving WhatsApp Image 2025-09-09 at 20.11.01_38463ce0.jpg to WhatsApp Image 2025-09-09 at 20.11.01_38463ce0.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RAW_DATA_DIR' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2040593585.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/{filename}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{RAW_DATA_DIR}/{filename}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Moving {src} → {dst}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Use shutil.move to handle cross-device links (copy then delete)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RAW_DATA_DIR' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e5914b",
      "metadata": {
        "id": "19e5914b"
      },
      "source": [
        "\n",
        "## SECTION 3: Extraction utilities\n",
        "\n",
        "Below we define helper functions for PDF, DOCX and PPTX text extraction. These are intentionally simple and well-commented.\n",
        "For scanned documents you will need an OCR pipeline (Tesseract or PaddleOCR) which is optional and not included by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5691bf58",
      "metadata": {
        "id": "5691bf58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "7e21ccba-c13b-4719-d442-d2857d682779"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pypdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-770682623.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extraction helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpypdf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPdfReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDocxDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpptx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPresentation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPptxPresentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pypdf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "# Extraction helpers\n",
        "from pypdf import PdfReader\n",
        "from docx import Document as DocxDocument\n",
        "from pptx import Presentation as PptxPresentation\n",
        "import os\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(path):\n",
        "    \"\"\"Extract text from a text-based PDF using pypdf (fast for native PDFs).\n",
        "    If the PDF is scanned, you'll need OCR (not included here).\n",
        "    \"\"\"\n",
        "    text_parts = []\n",
        "    reader = PdfReader(path)\n",
        "    for i, page in enumerate(reader.pages):\n",
        "        try:\n",
        "            page_text = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            page_text = \"\"\n",
        "        text_parts.append(f\"\\n--- PAGE {i+1} ---\\n\" + page_text)\n",
        "    return \"\\n\".join(text_parts)\n",
        "\n",
        "\n",
        "def extract_text_from_docx(path):\n",
        "    doc = DocxDocument(path)\n",
        "    paragraphs = [p.text for p in doc.paragraphs]\n",
        "    return \"\\n\".join(paragraphs)\n",
        "\n",
        "\n",
        "def extract_text_from_pptx(path):\n",
        "    prs = PptxPresentation(path)\n",
        "    slides_text = []\n",
        "    for si, slide in enumerate(prs.slides):\n",
        "        parts = []\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, 'text') and shape.text:\n",
        "                parts.append(shape.text)\n",
        "        slide_text = \"\\n\".join(parts)\n",
        "        slides_text.append(f\"\\n--- SLIDE {si+1} ---\\n\" + slide_text)\n",
        "    return \"\\n\".join(slides_text)\n",
        "\n",
        "print('Extraction helpers defined.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "517ebd50",
      "metadata": {
        "id": "517ebd50"
      },
      "source": [
        "\n",
        "## SECTION 4: Cleaning and chunking utilities\n",
        "\n",
        "We perform simple cleaning and chunking. The chunker below is character-based and suitable for prototyping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059a8892",
      "metadata": {
        "id": "059a8892"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Normalize whitespace and remove long runs of newlines\n",
        "    text = text.replace('\\r\\n', '\\n')\n",
        "    text = re.sub('\\n{3,}', '\\n\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"Return list of (chunk_id, chunk_text). Character-based overlapping chunks.\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    idx = 0\n",
        "    L = len(text)\n",
        "    while start < L:\n",
        "        end = min(start + chunk_size, L)\n",
        "        chunk = text[start:end]\n",
        "        chunks.append((f'chunk_{idx}', chunk))\n",
        "        idx += 1\n",
        "        start = end - chunk_overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "    return chunks\n",
        "\n",
        "print('Cleaning & chunking utilities ready.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7125b6",
      "metadata": {
        "id": "9b7125b6"
      },
      "source": [
        "\n",
        "## SECTION 5: Embeddings + Chroma setup\n",
        "\n",
        "We use `sentence-transformers` + Chroma (local duckdb+parquet) for embeddings and indexing.\n",
        "For higher-quality embeddings, replace the model with `instructor-xl` or `bge-large` if you have access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f94226d",
      "metadata": {
        "id": "2f94226d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "EMBED_MODEL = 'all-MiniLM-L6-v2'  # small & fast for prototype\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "persist_dir = 'chroma_db'\n",
        "client = chromadb.Client(Settings(chroma_db_impl='duckdb+parquet', persist_directory=persist_dir))\n",
        "collection_name = 'jarvis_notes'\n",
        "try:\n",
        "    collection = client.get_collection(collection_name)\n",
        "except Exception:\n",
        "    collection = client.create_collection(name=collection_name)\n",
        "\n",
        "print('Embedding model and Chroma collection ready:', collection_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7802795b",
      "metadata": {
        "id": "7802795b"
      },
      "source": [
        "\n",
        "## SECTION 6: Ingest function\n",
        "\n",
        "This function implements: extraction -> cleaning -> chunking -> embedding -> index (Chroma).\n",
        "It returns an ingestion summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "644fa34a",
      "metadata": {
        "id": "644fa34a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import uuid, time\n",
        "\n",
        "def ingest_file(path, filename=None, course=None):\n",
        "    if filename is None:\n",
        "        filename = os.path.basename(path)\n",
        "    name, ext = os.path.splitext(filename.lower())\n",
        "    doc_id = str(uuid.uuid4())\n",
        "\n",
        "    # Extract text based on extension\n",
        "    if ext == '.pdf':\n",
        "        raw = extract_text_from_pdf(path)\n",
        "    elif ext == '.docx':\n",
        "        raw = extract_text_from_docx(path)\n",
        "    elif ext == '.pptx':\n",
        "        raw = extract_text_from_pptx(path)\n",
        "    else:\n",
        "        raise ValueError('Unsupported extension: ' + ext)\n",
        "\n",
        "    cleaned = clean_text(raw)\n",
        "    chunks = chunk_text(cleaned, chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "    ids, docs, metas, embs = [], [], [], []\n",
        "    t0 = time.time()\n",
        "    for idx, (_, chunk_text) in enumerate(chunks):\n",
        "        cid = f\"{doc_id}_chunk_{idx}\"\n",
        "        meta = {'document_id': doc_id, 'source_filename': filename, 'chunk_index': idx, 'course': course or ''}\n",
        "        emb = embedder.encode(chunk_text).tolist()\n",
        "        ids.append(cid); docs.append(chunk_text); metas.append(meta); embs.append(emb)\n",
        "\n",
        "    collection.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)\n",
        "    client.persist()\n",
        "    t1 = time.time()\n",
        "    return {'document_id': doc_id, 'filename': filename, 'num_chunks': len(chunks), 'time_seconds': t1-t0}\n",
        "\n",
        "print('Ingest function ready. Example: ingest_file(\"/path/to/file.pdf\")')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5bd61f",
      "metadata": {
        "id": "ce5bd61f"
      },
      "source": [
        "\n",
        "## SECTION 7: Retrieval + simple RAG assembly\n",
        "\n",
        "`ask(question)` will retrieve top-k chunks and assemble a prompt. Replace the 'call_model' placeholder with your preferred model call\n",
        "(e.g., Hugging Face Inference API or local quantized model call).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf14e03",
      "metadata": {
        "id": "4bf14e03"
      },
      "outputs": [],
      "source": [
        "\n",
        "def retrieve(query, n_results=3):\n",
        "    q_emb = embedder.encode(query).tolist()\n",
        "    results = collection.query(query_embeddings=[q_emb], n_results=n_results)\n",
        "    docs = results['documents'][0]\n",
        "    metas = results['metadatas'][0]\n",
        "    return list(zip(docs, metas))\n",
        "\n",
        "def assemble_prompt(question, retrieved):\n",
        "    prompt = 'You are JARVIS, a helpful assistant. Use the context below to answer the question.\\n\\n'\n",
        "    for i, (doc_text, meta) in enumerate(retrieved):\n",
        "        prompt += f\"[Context {i+1}] (source: {meta.get('source_filename')}, chunk: {meta.get('chunk_index')})\\n\"\n",
        "        prompt += doc_text[:800] + '\\n\\n'\n",
        "    prompt += '\\nQuestion: ' + question + '\\nAnswer:'\n",
        "    return prompt\n",
        "\n",
        "# Placeholder model call - replace this function with a call to a model (HF, OpenAI, local runner)\n",
        "def call_model(prompt):\n",
        "    # Example: return prompt for inspection. Replace with actual API call or local inference.\n",
        "    return 'MODEL_OUTPUT_PLACEHOLDER - replace call_model with actual model invocation.'\n",
        "\n",
        "\n",
        "def ask(question, n_results=3):\n",
        "    retrieved = retrieve(question, n_results=n_results)\n",
        "    prompt = assemble_prompt(question, retrieved)\n",
        "    answer = call_model(prompt)\n",
        "    return {'answer': answer, 'prompt': prompt, 'retrieved': retrieved}\n",
        "\n",
        "print('ask(question) ready. Try ask(\"What is X?\") after ingesting documents.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}