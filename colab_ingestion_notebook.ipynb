{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Storm00212/JARVIS/blob/main/colab_ingestion_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "930bb34e",
      "metadata": {
        "id": "930bb34e"
      },
      "source": [
        "\n",
        "# JARVIS RAG Ingestion Notebook (Colab-ready)\n",
        "\n",
        "**Purpose:** This notebook walks you through an end-to-end prototype ingestion pipeline that:\n",
        "- Accepts PDF / DOCX / PPTX documents\n",
        "- Extracts clean text (with optional OCR)\n",
        "- Splits documents into semantic chunks\n",
        "- Generates embeddings for chunks\n",
        "- Stores chunks + embeddings into a local Chroma vector store\n",
        "- Exposes a simple `ask(question)` function that uses retrieval + prompt assembly (RAG)\n",
        "\n",
        "**Notes & assumptions**\n",
        "- Designed for Google Colab interactive use.\n",
        "- Includes a sample path from this session: `/mnt/data/jarvis-ai.zip` which you can inspect or replace with your own uploads.\n",
        "- Each code cell includes detailed comments to help you follow along.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7dd3208",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7dd3208",
        "outputId": "71f402ad-2365-4625-d28f-761ec9e21f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencies installed (or already present).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# SECTION 1: Install required packages\n",
        "# Run this cell in Google Colab to install dependencies. It may take 1-2 minutes.\n",
        "!pip install --quiet pypdf python-docx python-pptx sentence-transformers chromadb langchain tiktoken\n",
        "print('Dependencies installed (or already present).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb1df0a",
      "metadata": {
        "id": "cdb1df0a"
      },
      "source": [
        "\n",
        "## SECTION 2: Upload files (use UI) or use sample path\n",
        "\n",
        "You can upload files interactively using the cell below, or skip upload and use the sample file `'/mnt/data/jarvis-ai.zip'` if present.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3451fc9",
      "metadata": {
        "id": "e3451fc9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Optional interactive upload (works in Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    uploaded_files = list(uploaded.keys())\n",
        "except Exception as e:\n",
        "    uploaded_files = []\n",
        "print('Uploaded files detected:', uploaded_files)\n",
        "\n",
        "# Sample path from this session (exists in environment where we built the scaffold):\n",
        "SAMPLE_PATH = '/mnt/data/jarvis-ai.zip'\n",
        "print('Sample path exists:', os.path.exists(SAMPLE_PATH))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e5914b",
      "metadata": {
        "id": "19e5914b"
      },
      "source": [
        "\n",
        "## SECTION 3: Extraction utilities\n",
        "\n",
        "Below we define helper functions for PDF, DOCX and PPTX text extraction. These are intentionally simple and well-commented.\n",
        "For scanned documents you will need an OCR pipeline (Tesseract or PaddleOCR) which is optional and not included by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5691bf58",
      "metadata": {
        "id": "5691bf58"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extraction helpers\n",
        "from pypdf import PdfReader\n",
        "from docx import Document as DocxDocument\n",
        "from pptx import Presentation as PptxPresentation\n",
        "import os\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(path):\n",
        "    \"\"\"Extract text from a text-based PDF using pypdf (fast for native PDFs).\n",
        "    If the PDF is scanned, you'll need OCR (not included here).\n",
        "    \"\"\"\n",
        "    text_parts = []\n",
        "    reader = PdfReader(path)\n",
        "    for i, page in enumerate(reader.pages):\n",
        "        try:\n",
        "            page_text = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            page_text = \"\"\n",
        "        text_parts.append(f\"\\n--- PAGE {i+1} ---\\n\" + page_text)\n",
        "    return \"\\n\".join(text_parts)\n",
        "\n",
        "\n",
        "def extract_text_from_docx(path):\n",
        "    doc = DocxDocument(path)\n",
        "    paragraphs = [p.text for p in doc.paragraphs]\n",
        "    return \"\\n\".join(paragraphs)\n",
        "\n",
        "\n",
        "def extract_text_from_pptx(path):\n",
        "    prs = PptxPresentation(path)\n",
        "    slides_text = []\n",
        "    for si, slide in enumerate(prs.slides):\n",
        "        parts = []\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, 'text') and shape.text:\n",
        "                parts.append(shape.text)\n",
        "        slide_text = \"\\n\".join(parts)\n",
        "        slides_text.append(f\"\\n--- SLIDE {si+1} ---\\n\" + slide_text)\n",
        "    return \"\\n\".join(slides_text)\n",
        "\n",
        "print('Extraction helpers defined.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "517ebd50",
      "metadata": {
        "id": "517ebd50"
      },
      "source": [
        "\n",
        "## SECTION 4: Cleaning and chunking utilities\n",
        "\n",
        "We perform simple cleaning and chunking. The chunker below is character-based and suitable for prototyping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059a8892",
      "metadata": {
        "id": "059a8892"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Normalize whitespace and remove long runs of newlines\n",
        "    text = text.replace('\\r\\n', '\\n')\n",
        "    text = re.sub('\\n{3,}', '\\n\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"Return list of (chunk_id, chunk_text). Character-based overlapping chunks.\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    idx = 0\n",
        "    L = len(text)\n",
        "    while start < L:\n",
        "        end = min(start + chunk_size, L)\n",
        "        chunk = text[start:end]\n",
        "        chunks.append((f'chunk_{idx}', chunk))\n",
        "        idx += 1\n",
        "        start = end - chunk_overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "    return chunks\n",
        "\n",
        "print('Cleaning & chunking utilities ready.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7125b6",
      "metadata": {
        "id": "9b7125b6"
      },
      "source": [
        "\n",
        "## SECTION 5: Embeddings + Chroma setup\n",
        "\n",
        "We use `sentence-transformers` + Chroma (local duckdb+parquet) for embeddings and indexing.\n",
        "For higher-quality embeddings, replace the model with `instructor-xl` or `bge-large` if you have access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f94226d",
      "metadata": {
        "id": "2f94226d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "EMBED_MODEL = 'all-MiniLM-L6-v2'  # small & fast for prototype\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "persist_dir = 'chroma_db'\n",
        "client = chromadb.Client(Settings(chroma_db_impl='duckdb+parquet', persist_directory=persist_dir))\n",
        "collection_name = 'jarvis_notes'\n",
        "try:\n",
        "    collection = client.get_collection(collection_name)\n",
        "except Exception:\n",
        "    collection = client.create_collection(name=collection_name)\n",
        "\n",
        "print('Embedding model and Chroma collection ready:', collection_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7802795b",
      "metadata": {
        "id": "7802795b"
      },
      "source": [
        "\n",
        "## SECTION 6: Ingest function\n",
        "\n",
        "This function implements: extraction -> cleaning -> chunking -> embedding -> index (Chroma).\n",
        "It returns an ingestion summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "644fa34a",
      "metadata": {
        "id": "644fa34a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import uuid, time\n",
        "\n",
        "def ingest_file(path, filename=None, course=None):\n",
        "    if filename is None:\n",
        "        filename = os.path.basename(path)\n",
        "    name, ext = os.path.splitext(filename.lower())\n",
        "    doc_id = str(uuid.uuid4())\n",
        "\n",
        "    # Extract text based on extension\n",
        "    if ext == '.pdf':\n",
        "        raw = extract_text_from_pdf(path)\n",
        "    elif ext == '.docx':\n",
        "        raw = extract_text_from_docx(path)\n",
        "    elif ext == '.pptx':\n",
        "        raw = extract_text_from_pptx(path)\n",
        "    else:\n",
        "        raise ValueError('Unsupported extension: ' + ext)\n",
        "\n",
        "    cleaned = clean_text(raw)\n",
        "    chunks = chunk_text(cleaned, chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "    ids, docs, metas, embs = [], [], [], []\n",
        "    t0 = time.time()\n",
        "    for idx, (_, chunk_text) in enumerate(chunks):\n",
        "        cid = f\"{doc_id}_chunk_{idx}\"\n",
        "        meta = {'document_id': doc_id, 'source_filename': filename, 'chunk_index': idx, 'course': course or ''}\n",
        "        emb = embedder.encode(chunk_text).tolist()\n",
        "        ids.append(cid); docs.append(chunk_text); metas.append(meta); embs.append(emb)\n",
        "\n",
        "    collection.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)\n",
        "    client.persist()\n",
        "    t1 = time.time()\n",
        "    return {'document_id': doc_id, 'filename': filename, 'num_chunks': len(chunks), 'time_seconds': t1-t0}\n",
        "\n",
        "print('Ingest function ready. Example: ingest_file(\"/path/to/file.pdf\")')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5bd61f",
      "metadata": {
        "id": "ce5bd61f"
      },
      "source": [
        "\n",
        "## SECTION 7: Retrieval + simple RAG assembly\n",
        "\n",
        "`ask(question)` will retrieve top-k chunks and assemble a prompt. Replace the 'call_model' placeholder with your preferred model call\n",
        "(e.g., Hugging Face Inference API or local quantized model call).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf14e03",
      "metadata": {
        "id": "4bf14e03"
      },
      "outputs": [],
      "source": [
        "\n",
        "def retrieve(query, n_results=3):\n",
        "    q_emb = embedder.encode(query).tolist()\n",
        "    results = collection.query(query_embeddings=[q_emb], n_results=n_results)\n",
        "    docs = results['documents'][0]\n",
        "    metas = results['metadatas'][0]\n",
        "    return list(zip(docs, metas))\n",
        "\n",
        "def assemble_prompt(question, retrieved):\n",
        "    prompt = 'You are JARVIS, a helpful assistant. Use the context below to answer the question.\\n\\n'\n",
        "    for i, (doc_text, meta) in enumerate(retrieved):\n",
        "        prompt += f\"[Context {i+1}] (source: {meta.get('source_filename')}, chunk: {meta.get('chunk_index')})\\n\"\n",
        "        prompt += doc_text[:800] + '\\n\\n'\n",
        "    prompt += '\\nQuestion: ' + question + '\\nAnswer:'\n",
        "    return prompt\n",
        "\n",
        "# Placeholder model call - replace this function with a call to a model (HF, OpenAI, local runner)\n",
        "def call_model(prompt):\n",
        "    # Example: return prompt for inspection. Replace with actual API call or local inference.\n",
        "    return 'MODEL_OUTPUT_PLACEHOLDER - replace call_model with actual model invocation.'\n",
        "\n",
        "\n",
        "def ask(question, n_results=3):\n",
        "    retrieved = retrieve(question, n_results=n_results)\n",
        "    prompt = assemble_prompt(question, retrieved)\n",
        "    answer = call_model(prompt)\n",
        "    return {'answer': answer, 'prompt': prompt, 'retrieved': retrieved}\n",
        "\n",
        "print('ask(question) ready. Try ask(\"What is X?\") after ingesting documents.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}